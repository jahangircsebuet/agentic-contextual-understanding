import pandas as pd
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

class ResearchEvaluator:
    def __init__(self):
        self.judge_llm = ChatOpenAI(model="gpt-4o", temperature=0)
        
    def get_evaluation_prompt(self):
        return PromptTemplate.from_template("""
        You are a Senior Social Science Researcher. Evaluate the following 5W1H Master Summary 
        generated by a multi-agent system based on a social media post.
        
        SUMMARY TO EVALUATE:
        {summary}
        
        SCORING CRITERIA (Rate 1-5):
        1. Contextual Depth: Does it go beyond the text to explain the 'WHO' and 'WHEN'?
        2. Coherence: Is it a single narrative or just 5 disconnected bullet points?
        3. Hallucination Check: Does the 'WHEN' module invent news that didn't happen?
        
        Provide your response in this exact format:
        Depth: [score]
        Coherence: [score]
        Hallucination: [score]
        Rationale: [brief explanation]
        """)

    def evaluate_result(self, summary_text):
        prompt = self.get_evaluation_prompt().format(summary=summary_text)
        response = self.judge_llm.invoke(prompt)
        return response.content

# Example usage in main.py loop:
# scores = evaluator.evaluate_result(crew_output)